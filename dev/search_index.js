var documenterSearchIndex = {"docs":
[{"location":"rl/#Fitting-a-Q-Learner","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"","category":"section"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"We define a Q-Learner that explores an environment with 10 states and 3 actions with softmax policy.","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"using FitPopulations\nusing ConcreteStructs\nimport LogExpFunctions: softplus, logistic\n\n@concrete struct QLearner\n    q\nend\nQLearner() = QLearner(zeros(10, 3))\n\nfunction FitPopulations.initialize!(m::QLearner, parameters)\n    m.q .= parameters.q₀\nend\n\nFitPopulations.parameters(::QLearner) = (; q₀ = zeros(10, 3), η = 0., β_real = 1., γ_logit = 10.)\n\nfunction FitPopulations.logp(data, m::QLearner, parameters)\n    initialize!(m, parameters)\n    (; η, β_real, γ_logit) = parameters\n    β = softplus(β_real)\n    γ = logistic(γ_logit)\n    q = m.q\n    logp = 0.\n    for (; s, a, s′, r, done) in data\n        logp += logsoftmax(β, q, s, a)\n        td_error = r + γ * findmax(view(q, s′, :))[1] - q[s, a]\n        q[s, a] += η * td_error\n    end\n    logp\nend\n\nfunction FitPopulations.sample(rng, data, m::QLearner, parameters; environment)\n    (; η, β_real) = parameters\n    q = m.q\n    (; s′, done) = data[end]\n    if done\n        s′ = initial_state(rng, environment)\n    end\n    a′ = randsoftmax(rng, softplus(β_real), q, s′)\n    s′′ = transition(rng, environment, s′, a′)\n    r′ = reward(rng, environment, s′, a′, s′′)\n    (s = s′, a = a′, s′ = s′′, r = r′, done = isdone(rng, environment, s′′))\nend","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"Let us define the helper functions logsoftmax, randsoftmax and the environment functions initial_state, transition, reward, isdone.","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"using Distributions, LinearAlgebra\n\nlogsoftmax(β, q, s, a) = β * q[s, a] - logsumexp(β, view(q, s, :))\nfunction logsumexp(β, v)\n    m = β * maximum(v)\n    sumexp = zero(eltype(v))\n    for vᵢ in v\n        sumexp += exp(β * vᵢ - m)\n    end\n    m + log(sumexp)\nend\nfunction randsoftmax(rng, β, q, s)\n    m = maximum(view(q, s, :))\n    p = exp.(q[s, :] .- m)\n    p ./= sum(p)\n    rand(rng, Categorical(p))\nend\n@concrete struct Environment\n    t\n    r\nend\nfunction Environment(; rng = Random.default_rng())\n    Environment([normalize(rand(rng, 10), 1) for s in 1:10, a in 1:3],\n                randn(rng, 10, 3))\nend\ninitial_state(rng, ::Environment) = rand(rng, 1:10)\ntransition(rng, e::Environment, s, a) = rand(rng, Categorical(e.t[s, a]))\nreward(rng, e::Environment, s, a, s′) = e.r[s, a]\nisdone(rng, e::Environment, s) = s == 10","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"With this we can generate some artificial data and fit it.","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"using ComponentArrays, Random\n\nmodel = QLearner()\np = ComponentArray(parameters(model))\np.η = .15\np.β_real = 1.6\np.γ_logit = 1.8\nrng = Xoshiro(17)\ntmp = [simulate(model, p;\n                n_steps = 200, rng,\n                init = [(s = 1, a = 1, s′ = 1, r = 0., done = false)],\n                environment = Environment(; rng))\n       for _ in 1:100]\ndata = first.(tmp)\ndata_logp = sum(last.(tmp))","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"This is the probability with which the data was generated. Let us check the data probability under the default parameters.","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"population_model = PopulationModel(model, shared = (:q₀, :η, :β_real, :γ_logit))\np0 = ComponentArray(parameters(population_model))\nmc_marginal_logp(data, population_model, p0)","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"We see that the data probability under the default parameters is higher than under the parameter with which the data was generated. Now we maximize the log-likelihood, to find the optimal parameters for this data.","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"result = maximize_logp(data, population_model)\nresult.logp","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"The resulting data probability is indeed the highest. The fitted parameters are, however, not super close to p:","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"result.population_parameters","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"We can try fixing q₀:","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"result2 = maximize_logp(data, population_model, fixed = (; q₀ = zeros(10, 3)))\nresult2.logp","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"The data probability is a bit lower, as expected.","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"result2.population_parameters","category":"page"},{"location":"rl/","page":"Fitting a Q-Learner","title":"Fitting a Q-Learner","text":"The fitted parameters, however, are closer to p.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Functions-to-extend-with-a-new-model","page":"Reference","title":"Functions to extend with a new model","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.initialize!\nFitPopulations.parameters\nFitPopulations.logp\nFitPopulations.sample","category":"page"},{"location":"reference/#FitPopulations.initialize!","page":"Reference","title":"FitPopulations.initialize!","text":"initialize!(model, parameters)\n\nInitalizes the state of the model.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FitPopulations.parameters","page":"Reference","title":"FitPopulations.parameters","text":"parameters(model)\n\nReturns a named tuple.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FitPopulations.logp","page":"Reference","title":"FitPopulations.logp","text":"logp(data, model, parameters)\n\nReturns log P(datamodel parameters).\n\n\n\n\n\n","category":"function"},{"location":"reference/#FitPopulations.sample","page":"Reference","title":"FitPopulations.sample","text":"sample(rng, data, model, parameters; kw...)\n\nTakes already generated data as input and returns a new data point. This function is called in the simulate function. Keyword arguments are passed through the simulate function.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Population-model","page":"Reference","title":"Population model","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.PopulationModel","category":"page"},{"location":"reference/#FitPopulations.PopulationModel","page":"Reference","title":"FitPopulations.PopulationModel","text":"PopulationModel(model; prior = DiagonalNormalPrior(), shared = ())\n\nWrap a model for estimating population parameters. Shared parameters should be given as a tuple of symbols.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Fitting","page":"Reference","title":"Fitting","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.maximize_logp","category":"page"},{"location":"reference/#FitPopulations.maximize_logp","page":"Reference","title":"FitPopulations.maximize_logp","text":"maximize_logp(data, model, parameters = parameters(model);\n              fixed = (;)\n              coupled = [],\n              optimizer = default_optimizer(model, parameters, fixed),\n              lambda_l2 = 0.,\n              hessian_ad = Val(:ForwardDiff),\n              gradient_ad = Val(:Enzyme),\n              evaluate_training = false,\n              evaluate_test_data = nothing,\n              evaluation_trigger = EventTrigger(),\n              evaluation_options = (;),\n              callbacks = [],\n              verbosity = 1, print_interval = 3,\n              return_g! = false,\n              )\n\nSee also Callback.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Callbacks","page":"Reference","title":"Callbacks","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.Callback\nFitPopulations.LogProgress\nFitPopulations.Evaluator\nFitPopulations.CheckPointSaver\nFitPopulations.TimeTrigger\nFitPopulations.IterationTrigger\nFitPopulations.EventTrigger","category":"page"},{"location":"reference/#FitPopulations.Callback","page":"Reference","title":"FitPopulations.Callback","text":"Callback(trigger, function)\n\nCallback((trigger1, trigger2, ...), function)\n\nSee triggers IterationTrigger, TimeTrigger, EventTrigger. For callback functions see LogProgress, Evaluator, CheckPointSaver.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.LogProgress","page":"Reference","title":"FitPopulations.LogProgress","text":"LogProgress()\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.Evaluator","page":"Reference","title":"FitPopulations.Evaluator","text":"Evaluator(data, model, label = :evaluation, kw...)\n\nEvaluate logp or mc_marginal_logp on data, model and current parameters. Keyword arguments kw are passed to logp or mc_marginal_logp. Results are saved with the given label.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.CheckPointSaver","page":"Reference","title":"FitPopulations.CheckPointSaver","text":"CheckPointSaver(filename; overwrite = false)\n\nSaves checkpoints as JLD2 files.\n\nExamples\n\n# save every hour and at the end of the simulation.\nCallback((TimeTrigger(3600), EventTrigger((:end,))), CheckPointSaver(\"fit_results.jld2\"))\n# save at the start and at the end of the simulation.\nCallback(EventTrigger((:start, :end)), CheckPointSaver(\"fit_results.jld2\"))\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.TimeTrigger","page":"Reference","title":"FitPopulations.TimeTrigger","text":"TimeTrigger(Δt)\n\nTriggers every Δt seconds.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.IterationTrigger","page":"Reference","title":"FitPopulations.IterationTrigger","text":"IterationTrigger(Δi)\n\nTriggers every Δi iterations.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.EventTrigger","page":"Reference","title":"FitPopulations.EventTrigger","text":"EventTrigger(events = (:start, :start_finetuner, :start_fallback, :iteration_end, :end))\n\nTriggers at given events.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Optimizers","page":"Reference","title":"Optimizers","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.Optimizer\nFitPopulations.NLoptOptimizer\nFitPopulations.OptimOptimizer\nFitPopulations.OptimisersOptimizer\nFitPopulations.LaplaceEM","category":"page"},{"location":"reference/#FitPopulations.Optimizer","page":"Reference","title":"FitPopulations.Optimizer","text":"Optimizer(; optimizer = OptimOptimizer(Optim.LBFGS()), finetuner = nothing, fallback = OptimisersOptimizer())\n\nDefault optimizer. finetuner can be another optimizer that is called after the first optimizer finished. The fallback optimizer is called, if the optimizer or finetuner fails.\n\nCan also be constructed as\n\nOptimizer(optimizer; finetuner = nothing, fallback = OptimisersOptimizer(), kw...)\n\nwhere optimizer can be a symbol (to use NLopt), or an optimiser from Optim or Optimiser. See also NLoptOptimizer, OptimOptimizer, OptimisersOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.NLoptOptimizer","page":"Reference","title":"FitPopulations.NLoptOptimizer","text":"NLoptOptimizer(optimizer; options...)\n\nThe optimizer is a symbol (e.g. :LD_LBGFS) as specified here. For options, see NLopt options.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.OptimOptimizer","page":"Reference","title":"FitPopulations.OptimOptimizer","text":"OptimOptimizer(optimizer; options...)\n\nOptimizer can be anything from subtypes.(subtypes(Optim.AbstractOptimizer)). For options, see Optim Options.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.OptimisersOptimizer","page":"Reference","title":"FitPopulations.OptimisersOptimizer","text":"OptimisersOptimizer(opt; maxeval = 10^5, maxtime = Inf, min_grad_norm = 1e-8, lower_bounds = -Inf, upper_bounds = Inf)\n\nOptimizer opt can be anything from subtypes(Optimisers.AbstractRule). Optimization stops, when the L2-norm of the gradient falls below min_grad_norm or maxeval or maxtime is reached. See also Optimisers.\n\n\n\n\n\n","category":"type"},{"location":"reference/#FitPopulations.LaplaceEM","page":"Reference","title":"FitPopulations.LaplaceEM","text":"LaplaceEM(model, Estep_optimizer = Optimizer(), derivative_threshold = 1e-3, iterations = 10, stopper = () -> false)\n\nImplements the Expectation-Maximization (EM) method with Laplace approximation, as described e.g. in Huys et al. (2012).\n\n\n\n\n\n","category":"type"},{"location":"reference/#Simulation","page":"Reference","title":"Simulation","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.simulate\nFitPopulations.logp_tracked","category":"page"},{"location":"reference/#FitPopulations.simulate","page":"Reference","title":"FitPopulations.simulate","text":"simulate(\n    model,\n    parameters;\n    n_steps,\n    stop,\n    init,\n    tracked,\n    rng,\n    kw...\n)\n\n\nReturns a named tuple (; data, logp). stop(data, i) is a boolean function that depends on the sequence of simulated data and the iteration counter i. If tracked = true the state of the model is saved for every step in the simulation. Additional keyword arguments kw are passed to the sample function.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FitPopulations.logp_tracked","page":"Reference","title":"FitPopulations.logp_tracked","text":"logp_tracked(data, model, parameters)\n\n\nReturns a names tuple (; history, logp). In the history the state of the model is saved for every step.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Evaluation","page":"Reference","title":"Evaluation","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.mc_marginal_logp\nFitPopulations.BIC_int","category":"page"},{"location":"reference/#FitPopulations.mc_marginal_logp","page":"Reference","title":"FitPopulations.mc_marginal_logp","text":"mc_marginal_logp(data, model::PopulationModel, params;\n                 repetitions = 20, n_samples = 10^4, rng = Random.default_rng())\n\nEstimate the marginal log probability of the data given a model by sampling from the population.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FitPopulations.BIC_int","page":"Reference","title":"FitPopulations.BIC_int","text":"BIC_int(data, model, params; kw...)\n\n\nEstimate the Bayesian Information Criterion by sampling from the population. Keyword arguments kw are passed to mc_marginal_logp.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Derivatives","page":"Reference","title":"Derivatives","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"FitPopulations.gradient_logp\nFitPopulations.hessian_logp","category":"page"},{"location":"reference/#FitPopulations.gradient_logp","page":"Reference","title":"FitPopulations.gradient_logp","text":"gradient_logp(data, model, parameters; ad = Val(:Enzyme))\n\nCompute the gradient of logp.\n\n\n\n\n\n","category":"function"},{"location":"reference/#FitPopulations.hessian_logp","page":"Reference","title":"FitPopulations.hessian_logp","text":"hessian_logp(data, model, parameters; ad = Val(:ForwardDiff))\n\nCompute the hessian of logp.\n\n\n\n\n\n","category":"function"},{"location":"#FitPopulations","page":"Home","title":"FitPopulations","text":"","category":"section"},{"location":"#Example","page":"Home","title":"Example","text":"","category":"section"},{"location":"#Model-Definition","page":"Home","title":"Model Definition","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For a sequence of binary values y = (y_1 ldots y_T) we define a habituating biased coin model with probability P(y) = prod_t=1^TP(y_tw_t-1) with w_t = w_t-1 + eta (y_t-1 - sigma(w_t-1)), where w_0 and eta are parameters of the model and sigma(w) = 1(1 + exp(-w)).","category":"page"},{"location":"","page":"Home","title":"Home","text":"We define the model HabituatingBiasedCoin with state variable w and extend the functions parameters, initialize!, logp and sample from FitPopulations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# import FitPopulations: parameters, initialize!, logp, sample # if you want to avoid writing FitPopulations.logp etc. \nusing FitPopulations\nusing ConcreteStructs, Distributions\n\n@concrete struct HabituatingBiasedCoin\n    w\nend\nHabituatingBiasedCoin() = HabituatingBiasedCoin(Base.RefValue(0.))\n\nfunction FitPopulations.initialize!(m::HabituatingBiasedCoin, parameters)\n    m.w[] = parameters.w₀\nend\n\nFitPopulations.parameters(::HabituatingBiasedCoin) = (; w₀ = 0., η = 0.)\n\nsigmoid(w) = 1/(1 + exp(-w))\n\nfunction FitPopulations.logp(data, m::HabituatingBiasedCoin, parameters)\n    FitPopulations.initialize!(m, parameters)\n    η = parameters.η\n    logp = 0.\n    for yₜ in data\n        ρ = sigmoid(m.w[])\n        logp += logpdf(Bernoulli(ρ), yₜ)\n        m.w[] += η * (yₜ - ρ)\n    end\n    logp\nend\n\nfunction FitPopulations.sample(rng, ::Any, m::HabituatingBiasedCoin, ::Any)\n    rand(rng) ≤ sigmoid(m.w[])\nend","category":"page"},{"location":"#Generating-data","page":"Home","title":"Generating data","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let us generate 5 sequences of 30 steps with this model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: simulate\n\nmodel = HabituatingBiasedCoin()\nparams = (; w₀ = .3, η = .1)\ndata = [simulate(model, params, n_steps = 30).data for _ in 1:5]","category":"page"},{"location":"#Fitting-a-single-model","page":"Home","title":"Fitting a single model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First we check, if gradients are properly computed for our model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: gradient_logp\ngradient_logp(data[1], model, params)","category":"page"},{"location":"","page":"Home","title":"Home","text":"If this fails, it is recommended to check that logp does not allocate, e.g. with","category":"page"},{"location":"","page":"Home","title":"Home","text":"using BenchmarkTools\n@benchmark FitPopulations.logp($(data[1]), $model, $params)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We also check if Hessians are properly computed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: hessian_logp\nhessian_logp(data[1], model, params)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This may fail, if the model is too restrictive in its type parameters.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If everything works fine we run the optimizer:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: maximize_logp\nresult = maximize_logp(data[1], model)","category":"page"},{"location":"","page":"Home","title":"Home","text":"To inspect the state of the fitted model we can run","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: logp_tracked\nlogp_tracked(data[1], model, result.parameters).history","category":"page"},{"location":"","page":"Home","title":"Home","text":"We can also fit with some fixed parameters.","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = maximize_logp(data[1], model, fixed = (; η = 0.))\nresult.parameters","category":"page"},{"location":"","page":"Home","title":"Home","text":"or with coupled parameters","category":"page"},{"location":"","page":"Home","title":"Home","text":"result = maximize_logp(data[1], model, coupled = [(:w₀, :η)])\nresult.parameters","category":"page"},{"location":"#Fitting-a-population-model","page":"Home","title":"Fitting a population model","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Now we fit all data samples with approximate EM, assuming a diagonal normal prior over the parameters.","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: PopulationModel\npop_model1 = PopulationModel(model)\nresult1 = maximize_logp(data, pop_model1)\nresult1.population_parameters","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let us compare this to a model where all samples are assumed to be generated from the same parameters, i.e. the variance of the normal prior is zero.","category":"page"},{"location":"","page":"Home","title":"Home","text":"pop_model2 = PopulationModel(model, shared = (:w₀, :η))\nresult2 = maximize_logp(data, pop_model2)\nresult2.population_parameters","category":"page"},{"location":"","page":"Home","title":"Home","text":"To compare the models we look at the approximate BIC","category":"page"},{"location":"","page":"Home","title":"Home","text":"import FitPopulations: BIC_int\n(model1 = BIC_int(data, pop_model1, result1.population_parameters, repetitions = 1),\n model2 = BIC_int(data, pop_model2, result2.population_parameters))","category":"page"},{"location":"","page":"Home","title":"Home","text":"We see that the second model without variance of the prior has the lower BIC. This is not surprising, given that the data was generated with identical parameters.","category":"page"}]
}

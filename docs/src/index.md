# LaplacianExpectationMaximization

This package implements the Expectation-Maximization (EM) algorithm with a Laplacian approximation for the E-step, as described e.g. [here](http://dx.doi.org/10.1371/journal.pcbi.1002410), for finding the maximum likelihood estimate of the parameter distribution of a model.

Assume some data ``\mathcal D = \{\boldsymbol x_1, \ldots, \boldsymbol x_n\}`` was generated by sampling independently from the conditional density ``\boldsymbol x_i\sim p(\boldsymbol x|\boldsymbol \theta_i)``, where ``\boldsymbol \theta_i`` are parameters sampled from a multivariate normal distribution with diagonal covariance ``\boldsymbol \theta_i\sim\mathcal N(\boldsymbol \theta; \boldsymbol \mu, \boldsymbol\sigma)``. This package allows to find the maximum likelihood estimates 
```math
\begin{align*}
\boldsymbol \mu^*, \boldsymbol \sigma^* = \arg\max_{\boldsymbol \mu, \boldsymbol\sigma}p(\mathcal D | \boldsymbol\mu, \boldsymbol \sigma) &= \arg\max_{\boldsymbol \mu, \boldsymbol\sigma}\sum_{i=1}^n\log(p(\boldsymbol x_i|\boldsymbol\mu, \boldsymbol\sigma))\\ &= \arg\max_{\boldsymbol \mu, \boldsymbol\sigma}\sum_{i=1}^n\log\left(\int p(\boldsymbol x_i|\boldsymbol \theta_i)\mathcal N(\boldsymbol\theta_i;\boldsymbol\mu, \boldsymbol\sigma)d\boldsymbol\theta_i\right)
\end{align*}
```
using the EM algorithm with Laplacian approximation for the E-step.

## Example

### Model Definition

For a sequence of binary values ``y = (y_1, \ldots, y_T)`` we define a habituating biased coin model with probability ``P(y) = \prod_{t=1}^TP(y_t|w_{t-1})`` with ``w_t = w_{t-1} + \eta (y_{t-1} - \sigma(w_{t-1}))``, where ``w_0`` and ``\eta`` are parameters of the model and ``\sigma(w) = 1/(1 + \exp(-w))``.

We define the model `HabituatingBiasedCoin` with state variable `w` and extend the functions `parameters, initialize!, logp` and `sample` from `LaplacianExpectationMaximization`.

```@example hbc
# import LaplacianExpectationMaximization: parameters, initialize!, logp, sample # if you want to avoid writing LaplacianExpectationMaximization.logp etc. 
using LaplacianExpectationMaximization
using ConcreteStructs, Distributions

@concrete struct HabituatingBiasedCoin
    w
end
HabituatingBiasedCoin() = HabituatingBiasedCoin(Base.RefValue(0.))

function LaplacianExpectationMaximization.initialize!(m::HabituatingBiasedCoin, parameters)
    m.w[] = parameters.w₀
end

LaplacianExpectationMaximization.parameters(::HabituatingBiasedCoin) = (; w₀ = 0., η = 0.)

sigmoid(w) = 1/(1 + exp(-w))

function LaplacianExpectationMaximization.logp(data, m::HabituatingBiasedCoin, parameters)
    LaplacianExpectationMaximization.initialize!(m, parameters)
    η = parameters.η
    logp = 0.
    for yₜ in data
        ρ = sigmoid(m.w[])
        logp += logpdf(Bernoulli(ρ), yₜ)
        m.w[] += η * (yₜ - ρ)
    end
    logp
end

function LaplacianExpectationMaximization.sample(rng, ::Any, m::HabituatingBiasedCoin, ::Any)
    rand(rng) ≤ sigmoid(m.w[])
end
```

### Generating data

Let us generate 5 sequences of 30 steps with this model.

```@example hbc
import LaplacianExpectationMaximization: simulate

model = HabituatingBiasedCoin()
params = (; w₀ = .3, η = .1)
data = [simulate(model, params, n_steps = 30).data for _ in 1:5]
```

### Fitting a single model

First we check, if gradients are properly computed for our model.

```@example hbc
import LaplacianExpectationMaximization: gradient_logp
gradient_logp(data[1], model, params)
```

If this fails, it is recommended to check that `logp` does not allocate, e.g. with
```@example hbc
using BenchmarkTools
@benchmark LaplacianExpectationMaximization.logp($(data[1]), $model, $params)
```

We also check if Hessians are properly computed.

```@example hbc
import LaplacianExpectationMaximization: hessian_logp
hessian_logp(data[1], model, params)
```

This may fail, if the model is too restrictive in its type parameters.

If everything works fine we run the optimizer:

```@example hbc
import LaplacianExpectationMaximization: maximize_logp
result = maximize_logp(data[1], model)
```

To inspect the state of the fitted model we can run
```@example hbc
import LaplacianExpectationMaximization: logp_tracked
logp_tracked(data[1], model, result.parameters).history
```

We can also fit with some fixed parameters.
```@example hbc
result = maximize_logp(data[1], model, fixed = (; η = 0.))
result.parameters
```
or with coupled parameters
```@example hbc
result = maximize_logp(data[1], model, coupled = [(:w₀, :η)])
result.parameters
```

### Fitting a population model

Now we fit all data samples with approximate EM, assuming a diagonal normal prior over the parameters.

```@example hbc
import LaplacianExpectationMaximization: PopulationModel
pop_model1 = PopulationModel(model)
result1 = maximize_logp(data, pop_model1)
result1.population_parameters
```

Let us compare this to a model where all samples are assumed to be generated from the same parameters, i.e. the variance of the normal prior is zero.

```@example hbc
pop_model2 = PopulationModel(model, shared = (:w₀, :η))
result2 = maximize_logp(data, pop_model2)
result2.population_parameters
```

To compare the models we look at the approximate BIC
```@example hbc
import LaplacianExpectationMaximization: BIC_int
(model1 = BIC_int(data, pop_model1, result1.population_parameters, repetitions = 1),
 model2 = BIC_int(data, pop_model2, result2.population_parameters))
```
We see that the second model without variance of the prior has the lower BIC. This is not surprising, given that the data was generated with identical parameters.
